{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "This  exercise uses the mdp tool box developped at INRAE\n",
    "    https://pymdptoolbox.readthedocs.io/en/latest/index.html\n",
    "        \n",
    "Install it (if not already installed) and import the toolbox and the examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import mdptoolbox, mdptoolbox.example, mdptoolbox.util\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Cleaning Robot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dead line : 1 st of december for both groups (note book + report describing your mdp, including a drawing)\n",
    "\n",
    "Consider a house-cleaning robot. It can be either in the living room or at its charging station - or out  of battery. The living room can be clean or dirty. So there are five states: LD(in the living room, dirty), LC(in the living room, clean), CD(at the charger, dirty), CC(at the charger, clean), O (out of power).\n",
    "\n",
    "\n",
    "####  \n",
    "When in the living room,    the robot  can either choose to clean or to charge. \n",
    "\n",
    "When in the charging station, the robot can either choose to clean or to keep charging.\n",
    "\n",
    "When out of power, any of the two actions (clean, charge) leads to the same results: staying out of power\n",
    "\n",
    "####  \n",
    "\n",
    "The reward for being  in a clean state is rc \n",
    "\n",
    "The reward for being in a dirty state is rd < rc at each time step\n",
    "\n",
    "The reward for being out of power is  costcrash  -  lower or equal to rd ; let us set it equal to rd  (the living rooms becomes dirty anyway)\n",
    " \n",
    "\n",
    "####  \n",
    "\n",
    "When  the robot decides to  clean,\n",
    "*  if it is in the living room, then it may become out of power with proba e\n",
    "*  if it is in the charging station, no risk of running out of power   \n",
    "*  cleaning a clean floor leaves it clean\n",
    "*  cleaning a dirty floor can sometimes fail (even is battery is ok) - let eps be the probability of fail of the cleaning\n",
    "     \n",
    "When  the robot decides to  charge,  action charge always takes the robot to the charging station. The  dirtiness of the room can go from clean to dirty with a probability  pDust  and it stays for sure at the dirty level if dirty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Goal\n",
    "\n",
    "Model the problem by a (infinite horizon) MDP (describe your model in details) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the subject, we can model the following MDP:\n",
    "\n",
    "![RÃ©seau MDP](https://i.imgur.com/uZZ9Rjk.jpg)\n",
    "\n",
    "[See Original here](https://imgur.com/a/B9TAXE6)\n",
    "\n",
    "The following function details the construction of the transition and reward matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def model(e: float, eps: float, pDust: float, rc: float, rd: float, roop:float):\n",
    "    P = np.array([\n",
    "        # CL\n",
    "        [\n",
    "            # LC LD CC CD O\n",
    "            [ 1-e, 0, 0, 0, e ],         # LC\n",
    "            [ 1-e-eps, eps, 0, 0, e],    # LD\n",
    "            [ 1, 0, 0, 0, 0 ],           # CC\n",
    "            [ 0, 1, 0, 0, 0 ],           # CD\n",
    "            [ 0, 0, 0, 0, 1 ]            # O\n",
    "        ],\n",
    "\n",
    "        # CH\n",
    "        [\n",
    "            # LC LD CC CD O\n",
    "            [ 0, 0, 0, 1, 0 ],           # LC\n",
    "            [ 0, 0, 0, 1, 0 ],           # LD\n",
    "            [ 0, 0, 1-pDust, pDust, 0 ], # CC\n",
    "            [ 0, 0, 0, 1, 0 ],           # CD\n",
    "            [ 0, 0, 0, 0, 1 ]            # O\n",
    "        ]\n",
    "    ])\n",
    "\n",
    "    R = np.array([\n",
    "        # CL    CH\n",
    "        [ rc  , rc   ], # LC\n",
    "        [ rd  , rd   ], # LD\n",
    "        [ rc  , rc   ], # CC\n",
    "        [ rd  , rd   ], # CD\n",
    "        [ roop, roop ]  # O\n",
    "    ])\n",
    "\n",
    "    return (P, R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the best policy.\n",
    "\n",
    "* When the probability of runing out of battery is high\n",
    "* When it is low\n",
    "\n",
    "(explain the results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_policy(title: str, e: float, eps: float, pDust: float, rc: float, rd: float, roop:float):\n",
    "    P, R = model(e, eps, pDust, rc, rd, roop)\n",
    "    pi = mdptoolbox.mdp.PolicyIteration(P, R, 0.9)\n",
    "    # pi.setVerbose()\n",
    "    pi.run()\n",
    "\n",
    "    # print(P)\n",
    "    # print(R)\n",
    "    \n",
    "    txt = lambda x: \"CL\" if x == 0 else \"CH\"\n",
    "    eol = lambda x: \"\\n\" if x == len(pi.policy) - 1 else \" -> \"\n",
    "    \n",
    "    print(f\"[{title}]\")\n",
    "    print(f\"Policy: \", end=\"\")\n",
    "    for i, x in enumerate(pi.policy):\n",
    "        print(f\"{txt(x)}\", end=eol(i))\n",
    "\n",
    "    txt = lambda x: \"LC\" if x == 0 else \"LD\" if x == 1 else \"CC\" if x == 2 else \"CD\" if x == 3 else \"O\"\n",
    "    eol = lambda x: \"\\n\" if x == len(pi.V) - 1 else \" | \"\n",
    "    print(f\"Value: \", end=\"\")\n",
    "    for i, x in enumerate(pi.V):\n",
    "        print(f\"{txt(i)}: {x}\", end=eol(i))\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[high e]\n",
      "Policy: CL -> CL -> CH -> CL -> CL\n",
      "Value: LC: 1.0989010989010988 | LD: 0.08981936328051361 | CC: 5.301449307503799 | CD: 0.08083742695246225 | O: 0.0\n",
      "\n",
      "[low e]\n",
      "Policy: CL -> CL -> CH -> CL -> CL\n",
      "Value: LC: 5.263157894736843 | LD: 4.254076159116258 | CC: 7.07673773099167 | CD: 3.828668543204633 | O: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "e = 0.1\n",
    "eps = 0.01\n",
    "pDust = 0.1\n",
    "\n",
    "rc = 1\n",
    "rd = 0\n",
    "ro = rd # -1\n",
    "\n",
    "# with high probability of out of battery\n",
    "print_policy(\"high e\", 0.9, eps, pDust, rc, rd, ro)\n",
    "\n",
    "# with low probability of out of battery\n",
    "print_policy(\"low e\", 0.1, eps, pDust, rc, rd, ro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results here don't make much sense. It should been expected that with a higher likelyhood of running out of power, the agent would be more cautious and charge more frequently. However, the policies remain identical.\n",
    "\n",
    "This is probably due to an error in modeling, for which a solution has not been found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What policy is choosen if  the robot has a very good battery\n",
    "(i.e. the probability of being out  of charge is very low) ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Very low e]\n",
      "Policy: CL -> CL -> CL -> CL -> CL\n",
      "Value: LC: 9.174311926605506 | LD: 8.165230190984921 | CC: 9.256880733944955 | CD: 7.348707171886429 | O: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_policy(\"Very low e\", 0.01, eps, pDust, rc, rd, rd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the threat of being out of power is slim to none, the agent will spend all its time cleaning, as it guarantees a better reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What policy is choosen if the  living room gets dirty   quickly \n",
    "(i.e.  pdust increases) ?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[high pDust]\n",
      "Policy: CL -> CL -> CL -> CL -> CL\n",
      "Value: LC: 5.263157894736843 | LD: 4.254076159116259 | CC: 5.736842105263159 | CD: 3.828668543204633 | O: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_policy(\"high pDust\", e, eps, 0.9, rc, rd, ro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a higher pDust, the room will be dirty more often, this will encourage the agent to spend more time cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When e=0.1 does the best policy depend of rc ? of pDust? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fixed e, low pDust, low rc]\n",
      "Policy: CL -> CL -> CH -> CL -> CL\n",
      "Value: LC: 0.5263157894736844 | LD: 0.4254076159116259 | CC: 0.7076737730991671 | CD: 0.3828668543204633 | O: 0.0\n",
      "\n",
      "[Fixed e, high pDust, low rc]\n",
      "Policy: CL -> CL -> CL -> CL -> CL\n",
      "Value: LC: 0.5263157894736845 | LD: 0.42540761591162596 | CC: 0.573684210526316 | CD: 0.3828668543204633 | O: 0.0\n",
      "\n",
      "[Fixed e, low pDust, high rc]\n",
      "Policy: CL -> CL -> CH -> CL -> CL\n",
      "Value: LC: 52.63157894736844 | LD: 42.540761591162585 | CC: 70.76737730991671 | CD: 38.286685432046326 | O: 0.0\n",
      "\n",
      "[Fixed e, high pDust, high rc]\n",
      "Policy: CL -> CL -> CL -> CL -> CL\n",
      "Value: LC: 52.63157894736843 | LD: 42.54076159116258 | CC: 57.36842105263159 | CD: 38.286685432046326 | O: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_policy(\"Fixed e, low pDust, low rc\", 0.1, eps, 0.1, 0.1, rd, rd)\n",
    "print_policy(\"Fixed e, high pDust, low rc\", 0.1, eps, 0.9, 0.1, rd, rd)\n",
    "print_policy(\"Fixed e, low pDust, high rc\", 0.1, eps, 0.1, 10, rd, rd)\n",
    "print_policy(\"Fixed e, high pDust, high rc\", 0.1, eps, 0.9, 10, rd, rd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tell from the following examples, that when e is fixed, the optimal policy depends on pDust.\n",
    "Since pDust affects the probability that the agent will transition to a state with a lower reward (rd < rc), this makes sense. Also, being as rd is 0, changing rc whilst maintaning rd < rc will have no effect on the resulting policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if costcrash  is independent of rd?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Independant costcrash]\n",
      "Policy: CH -> CH -> CL -> CL -> CL\n",
      "Value: LC: 1.0 | LD: 0.0 | CC: 1.9 | CD: 0.0 | O: -10.000000000000002\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_policy(\"Independant costcrash\", 0.1, eps, pDust, rc, rd, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the threat of costcrash, our agent will be much more cautious and spend more time charging. However it doesn't make much sense that it does it all at the start instead of charging at intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Paste your python program here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Optional: extend your program so as to take into consideration several levels of battery (high, medium, low, very low for instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
